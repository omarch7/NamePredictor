{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Model\n",
    "---\n",
    "\n",
    "### Problem\n",
    "\n",
    "We need to classify small strings of less than 50 characters into two classes, Is a person name and is not a person name.\n",
    "\n",
    "For a traditonal Machine Learning approach, one of the biggest problems would be how to represent a short string of 2 or 3 words in a vector, using a *Bag of Words* approach will just create huge sparse vectors and this is not efficent, additionally some names might be unique and if we try to use all the words on the corpus we might end with thousands of features per vector.\n",
    "\n",
    "Instead we can treat this problem with a *Deep Learning* approach, not using word embeddings because we might not have an specific vector of a rare name or word and using word embeddings for two words in a sequence might be too much.\n",
    "\n",
    "### Approach\n",
    "\n",
    "Our dataset have strings with maximum 50 characters, knowing this we can build a recurrent neural network with LSTM cells to take each string as a sequence of characters, and we assign an id between 1 and 96 to each character in their respective position.\n",
    "\n",
    "From ASCII code *32* that is *\"Space\"* to *127* that is \"~\", between this range it covers all the latin characters used in the English language, punctuation and numbers.\n",
    "\n",
    "We set an offset of 31 so the indexes start from 1 and not 32.\n",
    "\n",
    "For simplicity we use **Keras** as the Deep Learning Framework, **TensorFlow** as the backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the 6 million samples with labels into a *Pandas DataFrame*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('full_names.csv', index_col=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding Strings\n",
    "\n",
    "To encode each string into numbers we just get the ASCII decimal code for each character and shift(substract) it by 31.\n",
    "\n",
    "For more information the ASCII table can be useful: [ASCII Table](http://www.rapidtables.com/code/text/ascii-table.htm)\n",
    "\n",
    "**Example**\n",
    "\n",
    "TYPE|String\n",
    "---|---\n",
    "CHAR|J|o|h|n| |S|m|i|t|h\n",
    "CODE|43|80|73|79|1|52|78|74|85|73\n",
    "\n",
    "Then for special characters like accented characters or specific language characters we just assigned 96, and not to forget that space is assgined as 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_string(s):\n",
    "    encoded = []\n",
    "    for c in s:\n",
    "        idx = ord(c)\n",
    "        if idx >= 32 and idx <= 126:\n",
    "            encoded.append(idx-31)\n",
    "        elif idx > 126:\n",
    "            # Rare Characters like accented letters and specific language characters\n",
    "            encoded.append(96)\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_vector(v):\n",
    "    decoded = []\n",
    "    for idx in v:\n",
    "        if idx > 0 and idx < 96:\n",
    "            decoded.append(chr(idx+31))\n",
    "        elif idx >= 96:\n",
    "            decoded.append('*')\n",
    "        else:\n",
    "            break\n",
    "    return \"\".join(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create indexes for all 6 million samples and then shuffle them so we can randomize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idxs = np.arange(data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All strings have different lengths (maximum 50 characters), so we use Keras helper *pad_sequences* function to add padding at the end of the vectors, so we can have 50 steps sequences, it just adds zeros at the end of the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "strings = pad_sequences(list(map(lambda s: encode_string(str(s)), data['string'].values[idxs])), maxlen=50, dtype=np.int32, padding='post')\n",
    "labels = data['is_person_name'].values[idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data 85% Train and 15% Test sets, I could have used the standard 30% or 40% for test, but 15% is already 900,000 strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(strings, labels, test_size=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried different hidden size and this one worked the best.\n",
    "\n",
    "The embedding size is set to 97, because we have 0 to 96 indexes, the Embedding layer will create one-hot encoded vectors for each step in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 97 # All accepted characters (0 Padding, 1-95 Common ASCII and 96 Rare Chars)\n",
    "HIDDEN_SIZE = 256\n",
    "INPUT_LENGTH = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras let me add an Embedding layer that will take a 2D vector of 256 x 50, and will output a Tensor of 256 x 50 x 97, this will be each batch, 0.3 dropout is added to the recurrent layer.\n",
    "\n",
    "Then the again before the fully connected layer we also add 0.3 of dropout. (Tried 0.5 before and this one worked the best)\n",
    "\n",
    "The fully connected layer has a sigmoid layer, i didn't use softmax because sigmoid works better for binary classification.\n",
    "\n",
    "Then we compute the loss according to the binary cross entropy and optimize with RMS Propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(EMBEDDING_SIZE, HIDDEN_SIZE, input_length=INPUT_LENGTH))\n",
    "model.add(LSTM(HIDDEN_SIZE, input_shape=(INPUT_LENGTH, HIDDEN_SIZE), recurrent_dropout=0.3))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the full train dataset has around 5 million samples, training on a local CPU is quite slow. \n",
    "\n",
    "I have setup a *Google Compute Engine* with a *K80 Tesla* GPU to train the whole network, I trained only for 5 epochs where it seems to converge and not improve anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5117349/5117349 [==============================] - 1975s - loss: 0.1782 - acc: 0.9311  \n",
      "Epoch 2/5\n",
      "5117349/5117349 [==============================] - 1951s - loss: 0.1263 - acc: 0.9545  \n",
      "Epoch 3/5\n",
      "5117349/5117349 [==============================] - 1951s - loss: 0.1234 - acc: 0.9561  - ETA: 2s -\n",
      "Epoch 4/5\n",
      "5117349/5117349 [==============================] - 1943s - loss: 0.1238 - acc: 0.9564  \n",
      "Epoch 5/5\n",
      "5117349/5117349 [==============================] - 1951s - loss: 0.1251 - acc: 0.9563  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3f30dab2e8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=256, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we evaluate the model with the test dataset and we score **96%** of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "902656/903062 [============================>.] - ETA: 0sTest loss:0.11065003591780606 - acc:0.9609938188136403\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, batch_size=512)\n",
    "print(\"Test loss:{} - acc:{}\".format(scores[0], scores[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we save the model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('models/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
